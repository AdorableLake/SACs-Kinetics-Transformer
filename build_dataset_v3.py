## build_dataset_v3.py
import pandas as pd
import numpy as np
import torch
import os
import shutil
from datetime import datetime
from data_encoder import CatalystFeatureProcessor

# --- 1. é…ç½® ---
METADATA_PATH = 'metadata.xlsx'
RAW_DATA_DIR = './data_raw'
PROCESSED_DIR = './processed_data'
# è¾“å‡ºæ–‡ä»¶åçš„å‰ç¼€
OUTPUT_PREFIX = 'catalyst_dataset_v3'

# ğŸ”¥ æ ¸å¿ƒï¼šé€šç”¨æ—¶é—´ç½‘æ ¼ (13ä¸ªç‚¹)
# æ¶µç›–äº† Gao (0-3), Cheng (0-8), Xu (0-60) çš„æ‰€æœ‰ç‰¹å¾åŒºé—´
TARGET_TIMES = np.array([0, 1, 2, 4, 6, 8, 10, 15, 20, 30, 40, 50, 60], dtype=np.float32)

def find_file(filename, search_path):
    """é€’å½’æŸ¥æ‰¾æ–‡ä»¶"""
    for root, dirs, files in os.walk(search_path):
        if filename in files:
            return os.path.join(root, filename)
    return None

def build_dataset():
    print(f"ğŸš€ å¯åŠ¨ V3 æ•°æ®èåˆå¼•æ“...")
    
    # 1. è¯»å– Excel
    try:
        df = pd.read_excel(METADATA_PATH)
        print(f"âœ… è¯»å–å…ƒæ•°æ®: {len(df)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
        return

    # 2. æ‹Ÿåˆç¼–ç å™¨
    processor = CatalystFeatureProcessor()
    
    # å»ºç«‹æ˜ å°„
    col_map = {
        'Catalyst_Type': 'Catalyst Type', 'Pollutant': 'Pollutant', 'Oxidant': 'PMS',
        'Anion_Type': 'Anion Type', 'pH': 'pH', 'Catalyst_Conc': 'Catalyst Conc',
        'Oxidant_Conc': 'PMS Conc', 'Pollutant_Conc_mgL': 'Pollutant Conc',
        'Anion_Conc_mM': 'Anion Conc', 'Temp_K': 'Temp'
    }
    
    # å‡†å¤‡ Fit æ•°æ®
    fit_data = []
    for _, row in df.iterrows():
        item = {}
        for code_key, excel_col in col_map.items():
            val = row.get(excel_col)
            if pd.isna(val): val = 0
            item[code_key] = val
        fit_data.append(item)
    
    processor.fit(pd.DataFrame(fit_data))
    print("âœ… ç‰¹å¾å¤„ç†å™¨æ‹Ÿåˆå®Œæˆã€‚")

    # 3. æ•°æ®èåˆä¸æ’å€¼
    X_list = []
    y_list = []
    valid_files = []
    missing_count = 0

    print("âš¡ï¸ å¼€å§‹æ‰§è¡Œå¤šæºæ•°æ®èåˆ (Interpolation)...")
    
    for idx, row in df.iterrows():
        filename = row['File Name']
        if not filename.endswith('.csv'): filename += '.csv'
        
        file_path = find_file(filename, RAW_DATA_DIR)
        
        if not file_path:
            print(f"âš ï¸  [ç¼ºå¤±] æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
            missing_count += 1
            continue
            
        try:
            # è¯»å–åŸå§‹æ›²çº¿
            csv_data = pd.read_csv(file_path, header=None)
            original_times = csv_data[0].values
            original_concs = csv_data[1].values
            
            # çº¿æ€§æ’å€¼ (æ ¸å¿ƒé­”æ³•)
            # Gaoçš„æ•°æ®(3minç»“æŸ)ä¼šè¢«è‡ªåŠ¨å»¶å±•ï¼ŒXuçš„æ•°æ®(60min)ä¼šè¢«ä¿ç•™é•¿å°¾
            interpolated_concs = np.interp(TARGET_TIMES, original_times, original_concs)
            y_seq = interpolated_concs.astype(np.float32)
            
        except Exception as e:
            print(f"âŒ [é”™è¯¯] è¯»å–å¤±è´¥ {filename}: {e}")
            continue

        # ç¼–ç ç‰¹å¾
        feature_dict = fit_data[idx]
        X_vec = processor.process_single_row(feature_dict).flatten()

        X_list.append(X_vec)
        y_list.append(y_seq)
        valid_files.append(filename)

    # 4. æ‰“åŒ…ä¿å­˜
    if len(X_list) == 0: 
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œé€€å‡ºã€‚")
        return

    X_tensor = torch.tensor(np.array(X_list), dtype=torch.float32)
    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).unsqueeze(-1)

    print(f"\nğŸ“Š V3 æ•°æ®é›†æŠ¥å‘Š:")
    
    # è‡ªåŠ¨ä»æ–‡ä»¶åä¸­æå–ä½œè€…å‰ç¼€ (å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º "author_xxx.csv")
    prefixes = set([f.split('_')[0] for f in valid_files])
    
    print(f"\nğŸ“Š V3 æ•°æ®é›†æŠ¥å‘Š:")
    print(f"   æ¥æºæ–‡çŒ®æ•°: {len(prefixes)} ({', '.join(prefixes)})") # <--- å˜èªæ˜äº†

    print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(X_list)}")
    print(f"   ç¼ºå¤±æ–‡ä»¶æ•°: {missing_count}")
    print(f"   æ—¶é—´ç½‘æ ¼: 0 -> 60 min (13 points)")
    print(f"   Tensorå½¢çŠ¶: X={X_tensor.shape}, y={y_tensor.shape}")

    dataset = {
        'X': X_tensor,
        'y': y_tensor,
        'filenames': valid_files, # ä¿å­˜æ–‡ä»¶åä»¥ä¾¿è¿½è¸ª
        'processor': processor,
        'times': TARGET_TIMES
    }
    
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    
    # --- åŒé‡ä¿å­˜ ---
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. å†å²å­˜æ¡£
    archive_path = os.path.join(PROCESSED_DIR, f'{OUTPUT_PREFIX}_{timestamp}.pt')
    torch.save(dataset, archive_path)
    
    # 2. æœ€æ–°ç‰ˆ (ä¾› train_v3.py è¯»å–)
    latest_path = os.path.join(PROCESSED_DIR, f'{OUTPUT_PREFIX}_latest.pt')
    torch.save(dataset, latest_path)
    
    print(f"ğŸ‰ æ•°æ®é›†ä¿å­˜å®Œæ¯•:")
    print(f"   ğŸ‘‰ å­˜æ¡£: {archive_path}")
    print(f"   ğŸ‘‰ æœ€æ–°: {latest_path}")

if __name__ == "__main__":
    build_dataset()